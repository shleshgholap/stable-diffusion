{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion from Scratch - Demo\n",
    "\n",
    "This notebook demonstrates the key components of our Stable Diffusion implementation:\n",
    "1. Noise scheduler visualization\n",
    "2. VAE encoding/decoding\n",
    "3. Text encoding with CLIP\n",
    "4. Diffusion sampling process\n",
    "5. Full text-to-image pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noise Scheduler Visualization\n",
    "\n",
    "The noise scheduler controls how noise is added during the forward diffusion process\n",
    "and removed during reverse diffusion (sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sd.schedulers.ddpm import DDPMScheduler\n",
    "from sd.schedulers.ddim import DDIMScheduler\n",
    "\n",
    "# Create schedulers with different beta schedules\n",
    "scheduler_linear = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    beta_schedule='linear'\n",
    ")\n",
    "\n",
    "scheduler_scaled = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule='scaled_linear'\n",
    ")\n",
    "\n",
    "# Plot beta schedules\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Betas\n",
    "axes[0].plot(scheduler_linear.betas.numpy(), label='Linear')\n",
    "axes[0].plot(scheduler_scaled.betas.numpy(), label='Scaled Linear')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('Beta')\n",
    "axes[0].set_title('Beta Schedule')\n",
    "axes[0].legend()\n",
    "\n",
    "# Alpha cumulative product (signal preservation)\n",
    "axes[1].plot(scheduler_linear.alphas_cumprod.numpy(), label='Linear')\n",
    "axes[1].plot(scheduler_scaled.alphas_cumprod.numpy(), label='Scaled Linear')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Alpha Cumprod')\n",
    "axes[1].set_title('Signal Preservation')\n",
    "axes[1].legend()\n",
    "\n",
    "# SNR (Signal-to-Noise Ratio)\n",
    "snr_linear = scheduler_linear.alphas_cumprod / (1 - scheduler_linear.alphas_cumprod)\n",
    "snr_scaled = scheduler_scaled.alphas_cumprod / (1 - scheduler_scaled.alphas_cumprod)\n",
    "axes[2].semilogy(snr_linear.numpy(), label='Linear')\n",
    "axes[2].semilogy(snr_scaled.numpy(), label='Scaled Linear')\n",
    "axes[2].set_xlabel('Timestep')\n",
    "axes[2].set_ylabel('SNR')\n",
    "axes[2].set_title('Signal-to-Noise Ratio')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Diffusion Visualization\n",
    "\n",
    "Visualize how an image is progressively noised during forward diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image\n",
    "def create_test_image(size=256):\n",
    "    \"\"\"Create a colorful test image.\"\"\"\n",
    "    x = np.linspace(-1, 1, size)\n",
    "    y = np.linspace(-1, 1, size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    R = (np.sin(X * 3) + 1) / 2\n",
    "    G = (np.sin(Y * 3) + 1) / 2\n",
    "    B = (np.sin(X * Y * 3) + 1) / 2\n",
    "    \n",
    "    img = np.stack([R, G, B], axis=0)\n",
    "    return torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # (1, 3, H, W)\n",
    "\n",
    "test_image = create_test_image(64).to(device)\n",
    "# Normalize to [-1, 1]\n",
    "test_image = test_image * 2 - 1\n",
    "\n",
    "# Add noise at different timesteps\n",
    "timesteps = [0, 100, 250, 500, 750, 999]\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='scaled_linear')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(3*len(timesteps), 3))\n",
    "\n",
    "for i, t in enumerate(timesteps):\n",
    "    noise = torch.randn_like(test_image)\n",
    "    noisy = scheduler.add_noise(test_image, noise, torch.tensor([t]))\n",
    "    \n",
    "    # Convert to displayable format\n",
    "    img = (noisy[0].cpu().permute(1, 2, 0).numpy() + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f't={t}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion Process')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VAE Encoding and Decoding\n",
    "\n",
    "The VAE compresses images to a lower-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sd.models.vae import VAE\n",
    "\n",
    "# Create a small VAE for demo\n",
    "vae = VAE(\n",
    "    in_channels=3,\n",
    "    latent_channels=4,\n",
    "    block_out_channels=(32, 64),\n",
    "    layers_per_block=1,\n",
    ").to(device)\n",
    "\n",
    "print(f'VAE parameters: {sum(p.numel() for p in vae.parameters()):,}')\n",
    "\n",
    "# Test encoding/decoding\n",
    "with torch.no_grad():\n",
    "    latents = vae.encode(test_image)\n",
    "    reconstructed = vae.decode(latents)\n",
    "\n",
    "print(f'Input shape: {test_image.shape}')\n",
    "print(f'Latent shape: {latents.shape}')\n",
    "print(f'Compression ratio: {test_image.numel() / latents.numel():.1f}x')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow((test_image[0].cpu().permute(1, 2, 0).numpy() + 1) / 2)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show latent channels\n",
    "latent_vis = latents[0].cpu().numpy()\n",
    "latent_grid = np.concatenate([latent_vis[i] for i in range(4)], axis=1)\n",
    "axes[1].imshow(latent_grid, cmap='viridis')\n",
    "axes[1].set_title('Latent (4 channels)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "recon = (reconstructed[0].cpu().permute(1, 2, 0).numpy() + 1) / 2\n",
    "axes[2].imshow(np.clip(recon, 0, 1))\n",
    "axes[2].set_title('Reconstructed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. U-Net Architecture\n",
    "\n",
    "The U-Net predicts noise from noisy latents, conditioned on timestep and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sd.models.unet import UNet, get_timestep_embedding\n",
    "\n",
    "# Create a small U-Net for demo\n",
    "unet = UNet(\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    model_channels=64,\n",
    "    num_res_blocks=1,\n",
    "    attention_resolutions=(4,),\n",
    "    channel_mult=(1, 2),\n",
    "    num_heads=4,\n",
    "    use_text_conditioning=True,\n",
    "    context_dim=64,\n",
    ").to(device)\n",
    "\n",
    "print(f'U-Net parameters: {sum(p.numel() for p in unet.parameters()):,}')\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 2\n",
    "latent_size = 8\n",
    "context_dim = 64\n",
    "seq_len = 10\n",
    "\n",
    "noisy_latents = torch.randn(batch_size, 4, latent_size, latent_size, device=device)\n",
    "timesteps = torch.randint(0, 1000, (batch_size,), device=device)\n",
    "context = torch.randn(batch_size, seq_len, context_dim, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise_pred = unet(noisy_latents, timesteps, context=context)\n",
    "\n",
    "print(f'Input shape: {noisy_latents.shape}')\n",
    "print(f'Output shape: {noise_pred.shape}')\n",
    "print(f'Timesteps: {timesteps.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Timestep Embedding Visualization\n",
    "\n",
    "Sinusoidal embeddings encode timestep information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timestep embeddings\n",
    "timesteps = torch.arange(0, 1000, 10)\n",
    "embeddings = get_timestep_embedding(timesteps, 64)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(embeddings.numpy().T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.title('Sinusoidal Timestep Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# Show that nearby timesteps have similar embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(embeddings.numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sim_matrix, cmap='viridis')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Timestep')\n",
    "plt.title('Embedding Similarity Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DDPM vs DDIM Sampling\n",
    "\n",
    "Compare stochastic (DDPM) and deterministic (DDIM) sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sampling trajectories\n",
    "ddpm = DDPMScheduler(num_train_timesteps=1000)\n",
    "ddim = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# Set different numbers of inference steps\n",
    "steps_list = [10, 25, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(steps_list), figsize=(4*len(steps_list), 3))\n",
    "\n",
    "for i, num_steps in enumerate(steps_list):\n",
    "    ddpm.set_timesteps(num_steps)\n",
    "    ddim.set_timesteps(num_steps)\n",
    "    \n",
    "    axes[i].plot(ddpm.timesteps.numpy(), label='DDPM', marker='o', markersize=3)\n",
    "    axes[i].plot(ddim.timesteps.numpy(), label='DDIM', marker='s', markersize=3)\n",
    "    axes[i].set_xlabel('Step')\n",
    "    axes[i].set_ylabel('Timestep')\n",
    "    axes[i].set_title(f'{num_steps} steps')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Sampling Timesteps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classifier-Free Guidance\n",
    "\n",
    "CFG combines conditional and unconditional predictions to improve sample quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sd.guidance.cfg import classifier_free_guidance\n",
    "\n",
    "# Simulate noise predictions\n",
    "noise_pred_cond = torch.randn(1, 4, 8, 8)\n",
    "noise_pred_uncond = torch.randn(1, 4, 8, 8)\n",
    "\n",
    "# Apply CFG with different guidance scales\n",
    "guidance_scales = [1.0, 3.0, 7.5, 15.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(guidance_scales), figsize=(3*len(guidance_scales), 6))\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "    guided = classifier_free_guidance(noise_pred_cond, noise_pred_uncond, scale)\n",
    "    \n",
    "    # Show one channel of conditional prediction\n",
    "    axes[0, i].imshow(noise_pred_cond[0, 0].numpy(), cmap='RdBu')\n",
    "    axes[0, i].set_title(f'Conditional')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Show guided prediction\n",
    "    axes[1, i].imshow(guided[0, 0].numpy(), cmap='RdBu')\n",
    "    axes[1, i].set_title(f'CFG scale={scale}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Classifier-Free Guidance Effect')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show how guidance scale affects magnitude\n",
    "scales = np.linspace(1, 20, 100)\n",
    "magnitudes = []\n",
    "\n",
    "for s in scales:\n",
    "    guided = classifier_free_guidance(noise_pred_cond, noise_pred_uncond, s)\n",
    "    magnitudes.append(guided.abs().mean().item())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(scales, magnitudes)\n",
    "plt.axvline(7.5, color='r', linestyle='--', label='Typical scale (7.5)')\n",
    "plt.xlabel('Guidance Scale')\n",
    "plt.ylabel('Mean Absolute Value')\n",
    "plt.title('CFG Magnitude vs Guidance Scale')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo covered the key components of Stable Diffusion:\n",
    "\n",
    "1. **Noise Scheduler**: Controls the diffusion process with beta schedules\n",
    "2. **VAE**: Compresses images to latent space (8x spatial reduction)\n",
    "3. **U-Net**: Predicts noise conditioned on timestep and text\n",
    "4. **Timestep Embedding**: Sinusoidal encoding of diffusion timestep\n",
    "5. **DDPM/DDIM**: Different sampling strategies (stochastic vs deterministic)\n",
    "6. **Classifier-Free Guidance**: Improves sample quality by combining predictions\n",
    "\n",
    "For full text-to-image generation, see the inference scripts in `scripts/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
